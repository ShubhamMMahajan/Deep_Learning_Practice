{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "##Comment these 2 lines out if you don't have a GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_dir = os.path.join(os.getcwd(), \"tomato_Google_Search\")\n",
    "tomato_data_path = os.path.join(tomato_dir,'*g')\n",
    "tomato_files = glob.glob(tomato_data_path)\n",
    "tomato_data = []\n",
    "for f1 in tomato_files:\n",
    "    img = cv2.imread(f1)\n",
    "    if img is not None:\n",
    "        img_pil = Image.fromarray(img)\n",
    "        img_pil_resize = img_pil.resize((100, 100))\n",
    "        tomato_data.append(np.array(img_pil_resize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "potato_dir = os.path.join(os.getcwd(), \"potato_Google_Search\")\n",
    "potato_data_path = os.path.join(potato_dir,'*g')\n",
    "potato_files = glob.glob(potato_data_path)\n",
    "potato_data = []\n",
    "for f1 in potato_files:\n",
    "    #img = cv2.imread(f1)\n",
    "    #potato_data.append(img)\n",
    "    img = cv2.imread(f1)\n",
    "    if img is not None:\n",
    "        img_pil = Image.fromarray(img)\n",
    "        img_pil_resize = img_pil.resize((100, 100))\n",
    "        potato_data.append(np.array(img_pil_resize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_label = [0] * len(tomato_data)\n",
    "potato_label = [1] * len(potato_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_image_train, tomato_image_test, tomato_label_train, tomato_label_test = train_test_split(tomato_data, tomato_label, test_size=0.3, random_state=20)\n",
    "potato_image_train, potato_image_test, potato_label_train, potato_label_test = train_test_split(potato_data, potato_label, test_size=0.3, random_state=20)\n",
    "\n",
    "training_images = np.array(tomato_image_train + potato_image_train)\n",
    "training_labels = np.array(tomato_label_train + potato_label_train)\n",
    "\n",
    "remaining_images = np.array(tomato_image_test + potato_image_test)\n",
    "remaining_labels = np.array(tomato_label_test + potato_label_test)\n",
    "\n",
    "validation_images, test_images, validation_labels, test_labels = train_test_split(remaining_images, remaining_labels, test_size=0.15, random_state=10)\n",
    "\n",
    "training_images, validation_images, test_images = training_images / 255.0, validation_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(training_images) + len(validation_images) + len(test_images) == len(tomato_data) + len(potato_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(100, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(200, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(200, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 98, 98, 100)       2800      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 49, 49, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 47, 47, 200)       180200    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 200)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 21, 21, 200)       360200    \n",
      "=================================================================\n",
      "Total params: 543,200\n",
      "Trainable params: 543,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 906 samples, validate on 331 samples\n",
      "Epoch 1/10\n",
      "864/906 [===========================>..] - ETA: 0s - loss: 0.9286 - accuracy: 0.7130"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(training_images, training_labels, epochs=10, \n",
    "                    validation_data=(validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(validation_images,  validation_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('test2.jpg')\n",
    "if img is not None:\n",
    "    img_pil = Image.fromarray(img)\n",
    "    img_pil_resize = img_pil.resize((100, 100))\n",
    "    print(model.predict_classes(np.reshape(np.array(img_pil_resize),[1,100,100,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_labels, model.predict_classes(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_converter = {0 : \"tomato\", 1 : \"potato\"}\n",
    "def convert_list(labels):\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = label_converter[labels[i]]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "y_actu = pd.Series(convert_list(list(test_labels)), name='Actual')\n",
    "y_pred = pd.Series(convert_list(list(model.predict_classes(test_images))), name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,6))  \n",
    "sns.heatmap(df_confusion, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
